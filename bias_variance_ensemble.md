
# ðŸŽ¯ Bias, Variance, and Ensemble Models â€“ Summary

## ðŸŽ¯ Bias and Variance

- **Bias** measures the accuracy of predictions.
  - Low bias â†’ Predictions are close to true values.
  - High bias â†’ Systematic errors, poor fit.

- **Variance** measures how predictions change with different training data.
  - Low variance â†’ Model is stable.
  - High variance â†’ Model is sensitive, overfits.

- To generalize well, models need:
  - Low bias and low variance.

## ðŸŽ¯ Bias-Variance Tradeoff

- As **model complexity** increases:
  - Bias decreases.
  - Variance increases.

- **Underfitting**: High bias, low variance.
- **Overfitting**: Low bias, high variance.
- The goal is to find a balance where the total error is minimized.

## ðŸŽ¯ Bagging (Bootstrap Aggregating)

- Combines predictions from multiple models trained on different bootstrap samples.
- Reduces **variance** without increasing bias too much.
- Example: **Random Forests**
  - Multiple shallow decision trees.
  - Trained in parallel.

## ðŸŽ¯ Boosting

- Trains models sequentially.
- Each model corrects the errors of the previous one.
- Reduces **bias** while slightly increasing variance.
- Examples:
  - **AdaBoost**
  - **Gradient Boosting**
  - **XGBoost**

## ðŸŽ¯ Summary Table

| Method    | Goal         | Learner Type | Training Style  |
|-----------|--------------|--------------|-----------------|
| Bagging   | Reduce Variance | High variance, low bias | Parallel         |
| Boosting  | Reduce Bias     | Low variance, high bias | Sequential       |
